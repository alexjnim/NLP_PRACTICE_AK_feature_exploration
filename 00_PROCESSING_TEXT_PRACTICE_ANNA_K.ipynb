{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Anna Karenina\\r\\n\\r\\nAuthor: Leo Tolstoy\\r\\n\\r\\nRelease Date: July 01, 1998 [EBook #1399]\\r\\nLast Updated: January 11, 2020\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF GUTENBERG EBOOK ANNA KARENINA ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced by David Brannan, Andrew Sly and David Widger.\\r'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "data = requests.get('http://www.gutenberg.org/files/1399/1399-h/1399-h.htm')\n",
    "content = data.content\n",
    "print(content[1930:2300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy families are all alike; every unhappy family is unhappy in its own way.\\nEverything was in confusion in the Oblonskys’ house. The wife had\\ndiscovered that the husband was carrying on an intrigue with a French girl, who\\nhad been a governess in their family, and she had announced to her husband that\\nshe could not go on living in the same house with him. This position of affairs\\nhad now lasted three days, and not only the husband and wife themselves, but\\nall the members of their family and household, were painfully conscious of it.\\nEvery person in the house felt that there was no sense in their living\\ntogether, and that the stray people brought together by chance in any inn had\\nmore in common with one another than they, the members of the family and\\nhousehold of the Oblonskys.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = clean_content[1932:2721]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenization\n",
    "\n",
    "### use nltk sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 5 \n",
      "\n",
      "Sample text sentences : \n",
      " ['Happy families are all alike; every unhappy family is unhappy in its own way.', 'Everything was in confusion in the Oblonskys’ house.', 'The wife had\\ndiscovered that the husband was carrying on an intrigue with a French girl, who\\nhad been a governess in their family, and she had announced to her husband that\\nshe could not go on living in the same house with him.', 'This position of affairs\\nhad now lasted three days, and not only the husband and wife themselves, but\\nall the members of their family and household, were painfully conscious of it.', 'Every person in the house felt that there was no sense in their living\\ntogether, and that the stray people brought together by chance in any inn had\\nmore in common with one another than they, the members of the family and\\nhousehold of the Oblonskys.']\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "print('Total sentences in sample_text:', len(sample_sentences), '\\n')\n",
    "print('Sample text sentences : \\n', sample_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now, as you can see, the tokenizer is quite intelligent. It doesn’t just use periods to delimit sentences,\n",
    "### but also considers other punctuation and capitalization of words. We can also tokenize text of other \n",
    "### languages using some pretrained models present in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or we can use the punkt sentence tokenizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 5 \n",
      "\n",
      "Sample text sentences : \n",
      " ['Happy families are all alike; every unhappy family is unhappy in its own way.'\n",
      " 'Everything was in confusion in the Oblonskys’ house.'\n",
      " 'The wife had\\ndiscovered that the husband was carrying on an intrigue with a French girl, who\\nhad been a governess in their family, and she had announced to her husband that\\nshe could not go on living in the same house with him.'\n",
      " 'This position of affairs\\nhad now lasted three days, and not only the husband and wife themselves, but\\nall the members of their family and household, were painfully conscious of it.'\n",
      " 'Every person in the house felt that there was no sense in their living\\ntogether, and that the stray people brought together by chance in any inn had\\nmore in common with one another than they, the members of the family and\\nhousehold of the Oblonskys.']\n"
     ]
    }
   ],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "print('Total sentences in sample_text:', len(sample_sentences), '\\n')\n",
    "print('Sample text sentences : \\n', np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regextokenizer\n",
    "\n",
    "The last tokenizer we cover in sentence tokenization is using an instance of the RegexpTokenizer class to tokenize text into sentences, where we will use specific regular expression-based patterns to segment sentences. The following snippet shows how to use a regex pattern to tokenize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 5 \n",
      "\n",
      "Sample text sentences : \n",
      " ['Happy families are all alike; every unhappy family is unhappy in its own way.'\n",
      " 'Everything was in confusion in the Oblonskys’ house.'\n",
      " 'The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him.'\n",
      " 'This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.'\n",
      " 'Every person in the house felt that there was no sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the members of the family and household of the Oblonskys.']\n"
     ]
    }
   ],
   "source": [
    "### remove \\n from text\n",
    "sample_text2 = sample_text.replace(\"\\n\", \" \")\n",
    "\n",
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(\n",
    "            pattern=SENTENCE_TOKENS_PATTERN,\n",
    "            gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text2)\n",
    "print('Total sentences in sample_text:', len(sample_sentences), '\\n')\n",
    "print('Sample text sentences : \\n', np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization \n",
    "\n",
    "### default nltk word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Happy', 'families', 'are', 'all', 'alike', ';', 'every',\n",
       "       'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way',\n",
       "       '.', 'Everything', 'was', 'in', 'confusion', 'in', 'the',\n",
       "       'Oblonskys', '’', 'house', '.', 'The', 'wife', 'had', 'discovered',\n",
       "       'that', 'the', 'husband', 'was', 'carrying', 'on', 'an',\n",
       "       'intrigue', 'with', 'a', 'French', 'girl', ',', 'who', 'had',\n",
       "       'been', 'a', 'governess', 'in', 'their', 'family', ',', 'and',\n",
       "       'she', 'had', 'announced', 'to', 'her', 'husband', 'that', 'she',\n",
       "       'could', 'not', 'go', 'on', 'living', 'in', 'the', 'same', 'house',\n",
       "       'with', 'him', '.', 'This', 'position', 'of', 'affairs', 'had',\n",
       "       'now', 'lasted', 'three', 'days', ',', 'and', 'not', 'only', 'the',\n",
       "       'husband', 'and', 'wife', 'themselves', ',', 'but', 'all', 'the',\n",
       "       'members', 'of', 'their', 'family', 'and', 'household', ',',\n",
       "       'were', 'painfully', 'conscious', 'of', 'it', '.', 'Every',\n",
       "       'person', 'in', 'the', 'house', 'felt', 'that', 'there', 'was',\n",
       "       'no', 'sense', 'in', 'their', 'living', 'together', ',', 'and',\n",
       "       'that', 'the', 'stray', 'people', 'brought', 'together', 'by',\n",
       "       'chance', 'in', 'any', 'inn', 'had', 'more', 'in', 'common',\n",
       "       'with', 'one', 'another', 'than', 'they', ',', 'the', 'members',\n",
       "       'of', 'the', 'family', 'and', 'household', 'of', 'the',\n",
       "       'Oblonskys', '.'], dtype='<U10')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toktoktokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Happy', 'families', 'are', 'all', 'alike', ';', 'every',\n",
       "       'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way.',\n",
       "       'Everything', 'was', 'in', 'confusion', 'in', 'the', 'Oblonskys',\n",
       "       '’', 'house.', 'The', 'wife', 'had', 'discovered', 'that', 'the',\n",
       "       'husband', 'was', 'carrying', 'on', 'an', 'intrigue', 'with', 'a',\n",
       "       'French', 'girl', ',', 'who', 'had', 'been', 'a', 'governess',\n",
       "       'in', 'their', 'family', ',', 'and', 'she', 'had', 'announced',\n",
       "       'to', 'her', 'husband', 'that', 'she', 'could', 'not', 'go', 'on',\n",
       "       'living', 'in', 'the', 'same', 'house', 'with', 'him.', 'This',\n",
       "       'position', 'of', 'affairs', 'had', 'now', 'lasted', 'three',\n",
       "       'days', ',', 'and', 'not', 'only', 'the', 'husband', 'and', 'wife',\n",
       "       'themselves', ',', 'but', 'all', 'the', 'members', 'of', 'their',\n",
       "       'family', 'and', 'household', ',', 'were', 'painfully',\n",
       "       'conscious', 'of', 'it.', 'Every', 'person', 'in', 'the', 'house',\n",
       "       'felt', 'that', 'there', 'was', 'no', 'sense', 'in', 'their',\n",
       "       'living', 'together', ',', 'and', 'that', 'the', 'stray', 'people',\n",
       "       'brought', 'together', 'by', 'chance', 'in', 'any', 'inn', 'had',\n",
       "       'more', 'in', 'common', 'with', 'one', 'another', 'than', 'they',\n",
       "       ',', 'the', 'members', 'of', 'the', 'family', 'and', 'household',\n",
       "       'of', 'the', 'Oblonskys', '.'], dtype='<U10')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "words = tokenizer.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regextokenizer\n",
    "\n",
    "We now look at how to use regular expressions and the RegexpTokenizer class to tokenize sentences into words. Remember that there are two main parameters that are useful in tokenization—the regex pattern for building the tokenizer and the gaps parameter, which, if set to true, is used to find the gaps between the tokens. Otherwise, it is used to find the tokens themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Happy', 'families', 'are', 'all', 'alike', 'every', 'unhappy',\n",
       "       'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', 'Everything',\n",
       "       'was', 'in', 'confusion', 'in', 'the', 'Oblonskys', 'house', 'The',\n",
       "       'wife', 'had', 'discovered', 'that', 'the', 'husband', 'was',\n",
       "       'carrying', 'on', 'an', 'intrigue', 'with', 'a', 'French', 'girl',\n",
       "       'who', 'had', 'been', 'a', 'governess', 'in', 'their', 'family',\n",
       "       'and', 'she', 'had', 'announced', 'to', 'her', 'husband', 'that',\n",
       "       'she', 'could', 'not', 'go', 'on', 'living', 'in', 'the', 'same',\n",
       "       'house', 'with', 'him', 'This', 'position', 'of', 'affairs', 'had',\n",
       "       'now', 'lasted', 'three', 'days', 'and', 'not', 'only', 'the',\n",
       "       'husband', 'and', 'wife', 'themselves', 'but', 'all', 'the',\n",
       "       'members', 'of', 'their', 'family', 'and', 'household', 'were',\n",
       "       'painfully', 'conscious', 'of', 'it', 'Every', 'person', 'in',\n",
       "       'the', 'house', 'felt', 'that', 'there', 'was', 'no', 'sense',\n",
       "       'in', 'their', 'living', 'together', 'and', 'that', 'the', 'stray',\n",
       "       'people', 'brought', 'together', 'by', 'chance', 'in', 'any',\n",
       "       'inn', 'had', 'more', 'in', 'common', 'with', 'one', 'another',\n",
       "       'than', 'they', 'the', 'members', 'of', 'the', 'family', 'and',\n",
       "       'household', 'of', 'the', 'Oblonskys'], dtype='<U10')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,\n",
    "                                gaps=False)\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'intrigue', 'affairs', 'families', 'announced', 'governess', 'with', 'stray', 'another', 'every', 'is', 'Oblonskys.', 'This', 'family,', 'girl,', 'had', 'position', 'they,', 'Oblonskys’', 'more', 'common', 'now', 'discovered', 'no', 'and', 'members', 'there', 'way.', 'together', 'who', 'were', 'could', 'been', 'person', 'alike;', 'house.', 'painfully', 'not', 'house', 'an', 'inn', 'together,', 'same', 'own', 'all', 'household,', 'a', 'felt', 'chance', 'themselves,', 'to', 'Every', 'one', 'carrying', 'that', 'any', 'husband', 'than', 'of', 'go', 'household', 'was', 'brought', 'Happy', 'people', 'but', 'three', 'their', 'French', 'she', 'by', 'unhappy', 'living', 'Everything', 'wife', 'on', 'the', 'in', 'lasted', 'him.', 'sense', 'family', 'days,', 'conscious', 'its', 'confusion', 'only', 'her', 'it.', 'The', 'are'},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern to identify tokens by using gaps between tokens\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,\n",
    "                                gaps=True)\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "np.array(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word punc tokenizer\n",
    "\n",
    "Besides the base RegexpTokenizer class , there are several derived classes that perform different types of word tokenization. The WordPunktTokenizer uses the pattern r'\\w+|[^\\w\\s]+' to tokenize sentences into independent alphabetic and non-alphabetic tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'intrigue', 'affairs', 'families', 'announced', 'governess', 'with', 'stray', 'another', 'every', 'is', 'it', 'This', 'had', 'position', 'alike', 'more', '’', 'common', 'now', 'discovered', 'no', 'and', 'members', 'there', 'together', 'who', 'were', 'could', 'been', 'person', 'him', 'painfully', 'not', 'house', 'an', 'inn', 'same', 'own', 'all', 'a', 'felt', 'chance', 'they', 'to', 'Every', 'one', 'carrying', 'that', 'any', 'way', 'husband', 'themselves', 'than', 'of', 'go', 'household', 'was', 'Happy', 'brought', 'people', 'but', 'three', 'their', 'girl', 'days', 'French', 'she', 'by', 'unhappy', ';', 'living', 'Everything', 'wife', 'on', 'the', 'in', 'lasted', ',', 'sense', 'family', 'conscious', 'its', 'confusion', 'only', 'her', '.', 'Oblonskys', 'The', 'are'},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sample_text)\n",
    "np.array(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whitespace tokenizer\n",
    "\n",
    "The WhitespaceTokenizer tokenizes sentences into words based on whitespace, like tabs, newlines, and spaces. The following snippet shows demonstrations of these tokenizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Happy', 'families', 'are', 'all', 'alike;', 'every', 'unhappy',\n",
       "       'family', 'is', 'unhappy', 'in', 'its', 'own', 'way.',\n",
       "       'Everything', 'was', 'in', 'confusion', 'in', 'the', 'Oblonskys’',\n",
       "       'house.', 'The', 'wife', 'had', 'discovered', 'that', 'the',\n",
       "       'husband', 'was', 'carrying', 'on', 'an', 'intrigue', 'with', 'a',\n",
       "       'French', 'girl,', 'who', 'had', 'been', 'a', 'governess', 'in',\n",
       "       'their', 'family,', 'and', 'she', 'had', 'announced', 'to', 'her',\n",
       "       'husband', 'that', 'she', 'could', 'not', 'go', 'on', 'living',\n",
       "       'in', 'the', 'same', 'house', 'with', 'him.', 'This', 'position',\n",
       "       'of', 'affairs', 'had', 'now', 'lasted', 'three', 'days,', 'and',\n",
       "       'not', 'only', 'the', 'husband', 'and', 'wife', 'themselves,',\n",
       "       'but', 'all', 'the', 'members', 'of', 'their', 'family', 'and',\n",
       "       'household,', 'were', 'painfully', 'conscious', 'of', 'it.',\n",
       "       'Every', 'person', 'in', 'the', 'house', 'felt', 'that', 'there',\n",
       "       'was', 'no', 'sense', 'in', 'their', 'living', 'together,', 'and',\n",
       "       'that', 'the', 'stray', 'people', 'brought', 'together', 'by',\n",
       "       'chance', 'in', 'any', 'inn', 'had', 'more', 'in', 'common',\n",
       "       'with', 'one', 'another', 'than', 'they,', 'the', 'members', 'of',\n",
       "       'the', 'family', 'and', 'household', 'of', 'the', 'Oblonskys.'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Robust Tokenizers with NLTK and spaCy\n",
    "For a typical NLP pipeline, I recommend leveraging state-of-the-art libraries like NLTK and spaCy and using some of their robust utilities to build a custom function to perform both sentence- and word-level tokenization. A simple example is depicted in the following snippets. We start with looking at how we can leverage NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.']),\n",
       "       list(['Everything', 'was', 'in', 'confusion', 'in', 'the', 'Oblonskys', '’', 'house', '.']),\n",
       "       list(['The', 'wife', 'had', 'discovered', 'that', 'the', 'husband', 'was', 'carrying', 'on', 'an', 'intrigue', 'with', 'a', 'French', 'girl', ',', 'who', 'had', 'been', 'a', 'governess', 'in', 'their', 'family', ',', 'and', 'she', 'had', 'announced', 'to', 'her', 'husband', 'that', 'she', 'could', 'not', 'go', 'on', 'living', 'in', 'the', 'same', 'house', 'with', 'him', '.']),\n",
       "       list(['This', 'position', 'of', 'affairs', 'had', 'now', 'lasted', 'three', 'days', ',', 'and', 'not', 'only', 'the', 'husband', 'and', 'wife', 'themselves', ',', 'but', 'all', 'the', 'members', 'of', 'their', 'family', 'and', 'household', ',', 'were', 'painfully', 'conscious', 'of', 'it', '.']),\n",
       "       list(['Every', 'person', 'in', 'the', 'house', 'felt', 'that', 'there', 'was', 'no', 'sense', 'in', 'their', 'living', 'together', ',', 'and', 'that', 'the', 'stray', 'people', 'brought', 'together', 'by', 'chance', 'in', 'any', 'inn', 'had', 'more', 'in', 'common', 'with', 'one', 'another', 'than', 'they', ',', 'the', 'members', 'of', 'the', 'family', 'and', 'household', 'of', 'the', 'Oblonskys', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "sents = tokenize_text(sample_text)\n",
    "np.array(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get to the level of word-level tokenization by leveraging list comprehensions, as depicted in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Happy', 'families', 'are', 'all', 'alike', ';', 'every',\n",
       "       'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way',\n",
       "       '.', 'Everything', 'was', 'in', 'confusion', 'in', 'the',\n",
       "       'Oblonskys', '’', 'house', '.', 'The', 'wife', 'had', 'discovered',\n",
       "       'that', 'the', 'husband', 'was', 'carrying', 'on', 'an',\n",
       "       'intrigue', 'with', 'a', 'French', 'girl', ',', 'who', 'had',\n",
       "       'been', 'a', 'governess', 'in', 'their', 'family', ',', 'and',\n",
       "       'she', 'had', 'announced', 'to', 'her', 'husband', 'that', 'she',\n",
       "       'could', 'not', 'go', 'on', 'living', 'in', 'the', 'same', 'house',\n",
       "       'with', 'him', '.', 'This', 'position', 'of', 'affairs', 'had',\n",
       "       'now', 'lasted', 'three', 'days', ',', 'and', 'not', 'only', 'the',\n",
       "       'husband', 'and', 'wife', 'themselves', ',', 'but', 'all', 'the',\n",
       "       'members', 'of', 'their', 'family', 'and', 'household', ',',\n",
       "       'were', 'painfully', 'conscious', 'of', 'it', '.', 'Every',\n",
       "       'person', 'in', 'the', 'house', 'felt', 'that', 'there', 'was',\n",
       "       'no', 'sense', 'in', 'their', 'living', 'together', ',', 'and',\n",
       "       'that', 'the', 'stray', 'people', 'brought', 'together', 'by',\n",
       "       'chance', 'in', 'any', 'inn', 'had', 'more', 'in', 'common',\n",
       "       'with', 'one', 'another', 'than', 'they', ',', 'the', 'members',\n",
       "       'of', 'the', 'family', 'and', 'household', 'of', 'the',\n",
       "       'Oblonskys', '.'], dtype='<U10')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for sentence in sents for word in sentence]\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way, we can leverage spaCy to perform sentence- and word-level tokenizations really quickly, as depicted in the following snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Happy families are all alike; every unhappy family is unhappy in its own way.\n",
       ",\n",
       "       Everything was in confusion in the Oblonskys’ house.,\n",
       "       The wife had\n",
       "discovered that the husband was carrying on an intrigue with a French girl, who\n",
       "had been a governess in their family, and she had announced to her husband that\n",
       "she could not go on living in the same house with him.,\n",
       "       This position of affairs\n",
       "had now lasted three days, and not only the husband and wife themselves, but\n",
       "all the members of their family and household, were painfully conscious of it.\n",
       ",\n",
       "       Every person in the house felt that there was no sense in their living\n",
       "together, and that the stray people brought together by chance in any inn had\n",
       ",\n",
       "       more in common with one another than they, the members of the family and\n",
       "household of the Oblonskys.], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', parse = True, tag=True, entity=True)\n",
    "text_spacy = nlp(sample_text)\n",
    "sents = np.array(list(text_spacy.sents))\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING ACCENTED CHARACTERS\n",
    "Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. This shows a simple example — converting é to e. The following function is a simple way of tackling this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')\n",
    "'Some Accented text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPANDING CONTRACTIONS\n",
    "\n",
    "Contractions are shortened versions of words or syllables. These exist in written and spoken forms. Shortened versions of existing words are created by removing specific letters and sounds. In the case of English contractions, they are often created by removing one of the vowels from the word. Examples include “is not” to “isn’t” and “will not” to “won’t”, where you can notice the apostrophe being used to denote the contraction and some of the vowels and other letters being removed.\n",
    "\n",
    "By nature, contractions pose a problem for NLP and text analytics because, to start with, we have a special apostrophe character in the word. Besides this, we also have two or more words represented by a contraction and this opens a whole new can of worms when we try to tokenize them or standardize the words. Hence, there should be some definite process for dealing with contractions when processing text.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all cannot expand contractions I would think'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"Y'all can't expand contractions I'd think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING SPECIAL CHARACTERS\n",
    "Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them. The following code helps us remove special characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\",\n",
    "                          remove_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumped over the big dog\n",
      "THE QUICK BROWN FOX JUMPED OVER THE BIG DOG\n",
      "The Quick Brown Fox Jumped Over The Big Dog\n"
     ]
    }
   ],
   "source": [
    "# lowercase\n",
    "text = 'The quick brown fox jumped over The Big Dog'\n",
    "print(text.lower())\n",
    "\n",
    "# uppercase\n",
    "print(text.upper())\n",
    "\n",
    "# title case\n",
    "print(text.title())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correcting repeating texts\n",
    "\n",
    "We will now utilize the WordNet corpus to check for valid words at each stage and terminate the loop once it is obtained. This introduces the semantic correction needed for our algorithm, as illustrated in the following snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Word: finalllyy\n",
      "Step: 2 Word: finallly\n",
      "Step: 3 Word: finally\n",
      "Final correct word: finally\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "while True:\n",
    "    # check for semantically correct word\n",
    "    if wordnet.synsets(old_word):\n",
    "        print(\"Final correct word:\", old_word)\n",
    "        break\n",
    "    # remove one repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution,\n",
    "                                  old_word)\n",
    "    if new_word != old_word:\n",
    "        print('Step: {} Word: {}'.format(step, new_word))\n",
    "        step += 1 # update step\n",
    "        # update old word to last substituted state\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Final word:\", new_word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a better version of this code by writing the logic in a function, as depicted here, to make it more generic to deal with incorrect tokens from a list of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My school is really amazing'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
    "' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming\n",
    "\n",
    "Word stems are also often known as the base form of a word and we can create new words by attaching affixes to them. This process is known as inflection. The reverse of this is obtaining the base form of a word from its inflected form and this is known as stemming. Consider the word “JUMP”, you can add affixes to it and form several new words like “JUMPS”, “JUMPED”, and “JUMPING”. In this case, the base word is “JUMP” and this is the word stem. If we were to carry out stemming on any of its three inflected forms, we would get the base form.\n",
    "\n",
    "Stemming helps us standardize words to their base stem irrespective of their inflections, which helps many applications like classifying or clustering text or even in information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'lie', 'strang')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# porter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'), ps.stem('lying'), ps.stem('strange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'lying', 'strange')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lancaster stemmer\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'), ls.stem('lying'), ls.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The RegexpStemmer uses regular expressions to identify the morphological affixes in words and any part of the string matching them is removed. You can see how the stemming results are different from the previous stemmers and is based completely on our custom defined rules based on regular expressions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'ly', 'strange')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'), rs.stem('lying'), rs.stem('strange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Porter stemmer is used most frequently, but you should choose your stemmer based on your problem and after trial and error. The following is a basic function that can be used for stemming text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If needed, you can build your own stemmer with your own defined rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEMATIZATION \n",
    "\n",
    "The process of lemmatization is very similar to stemming, where we remove word affixes to get to a base form of the word. However in this case, this base form is also known as the root word but not the root stem. The difference between the two is that the root stem may not always be a lexicographically correct word, i.e., it may not be present in the dictionary but the root word, also known as the lemma, will always be present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "\n",
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "\n",
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function basically finds the base form or lemma for a given word using the word and its part of speech by checking the WordNet corpus and uses a recursive technique for removing affixes from the word until a match is found in WordNet. If no match is found, the input word is returned unchanged. The part of speech is extremely important because if that is wrong, the lemmatization will not be effective, as you can see in the following snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy makes things a lot easier since it performs parts of speech tagging and effective lemmatization for each token in a text document without you worrying about if you are using lemmatization effectively. The following function can be leveraged for performing effective lemmatization, thanks to spaCy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash ! his crash yesterday , ours crash daily'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('-PRON-', 'crash')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nlp(text)\n",
    "test[0].lemma_, test[3].lemma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try this with our sample sentences from anna karenina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every person in the house felt that there was no sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the members of the family and household of the Oblonskys.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'every person in the house feel that there be no sense in their living together , and that the stray people bring together by chance in any inn have more in common with one another than they , the member of the family and household of the Oblonskys .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_text(sample_sentences[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "Stopwords are words that have little or no significance and are usually removed from text when processing it so as to retain words having maximum significance and context. Stopwords usually occur most frequently if you aggregate a corpus of text based on singular tokens and checked their frequencies. Words like “a,” “the,” “and,” and so on are stopwords. There is no universal or exhaustive list of stopwords and often each domain or language has its own set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', , stopwords , computer'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # this will remove the white spaces from the tokens\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "remove_stopwords(\"The, and, if are stopwords, computer is not\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every person house felt sense living together , stray people brought together chance inn common one another , members family household Oblonskys .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(sample_sentences[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no universal stopword list, but we use a standard English language stopwords list from NLTK. You can also add your own domain-specific stopwords as needed. In the previous function, we leverage the use of NLTK, which has a list of stopwords for English, and use it to filter out all tokens that correspond to stopwords. This output shows us a reduced number of tokens compared to what we had earlier and you can compare and check the tokens that were removed as stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRINGING IT ALL TOGETHER — BUILDING A TEXT NORMALIZER\n",
    "Let’s now bring everything we learned together and chain these operations to build a text normalizer to preprocess text data. We focus on including the major components often used for text wrangling in our custom function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "import unicodedata\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # this will remove the white spaces from the tokens\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True,\n",
    "                     text_lemmatization=True, special_char_removal=True,\n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions\n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits\n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
