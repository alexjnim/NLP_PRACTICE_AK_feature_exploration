{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 5 \n",
      "\n",
      "Sample text sentences : \n",
      " ['Happy families are all alike; every unhappy family is unhappy in its own way.'\n",
      " 'Everything was in confusion in the Oblonskys’ house.'\n",
      " 'The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him.'\n",
      " 'This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.'\n",
      " 'Every person in the house felt that there was no sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the members of the family and household of the Oblonskys.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "data = requests.get('http://www.gutenberg.org/files/1399/1399-h/1399-h.htm')\n",
    "content = data.content\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "sample_text = clean_content[1932:2721]\n",
    "sample_text2 = sample_text.replace(\"\\n\", \" \")\n",
    "sample_text2\n",
    "\n",
    "### remove \\n from text\n",
    "sample_text2 = sample_text.replace(\"\\n\", \" \")\n",
    "\n",
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(\n",
    "            pattern=SENTENCE_TOKENS_PATTERN,\n",
    "            gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text2)\n",
    "print('Total sentences in sample_text:', len(sample_sentences), '\\n')\n",
    "print('Sample text sentences : \\n', np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### example corpus and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Document Category\n",
       "0                                      The sky is blue and beautiful.  weather\n",
       "1                                   Love this blue and beautiful sky!  weather\n",
       "2                        The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
       "4                         I love green eggs, ham, sausages and bacon!     food\n",
       "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
       "6            The sky is very blue and the sky is very beautiful today  weather\n",
       "7                         The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a corpus of documents\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'\n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array(sample_sentences)\n",
    "corpus_df = pd.DataFrame({'Document': corpus})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the focus of this article is on feature engineering, we build a simple text preprocessor that focuses on removing special characters, extra whitespace, digits, stopwords, and then lowercasing the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['happy families alike every unhappy family unhappy way',\n",
       "       'everything confusion oblonskys house',\n",
       "       'wife discovered husband carrying intrigue french girl governess family announced husband could go living house',\n",
       "       'position affairs lasted three days husband wife members family household painfully conscious',\n",
       "       'every person house felt sense living together stray people brought together chance inn common one another members family household oblonskys'],\n",
       "      dtype='<U140')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def normalize_document(doc):\n",
    "    # lowercase and remove special characters\\whitespace\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words model\n",
    "\n",
    "The Bag of Words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0), or even weighted values. The model’s name is such because each document is represented literally as a bag of its own words, disregarding word order, sequences, and grammar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x44 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# get bag of words features in sparse format\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 22)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 41)\t2\n",
      "  (0, 16)\t1\n",
      "  (0, 42)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 31)\t1\n",
      "  (1, 23)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 23)\t1\n",
      "  (2, 43)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 25)\t2\n",
      "  (2, 5)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 21)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 29)\t1\n",
      "  :\t:\n",
      "  (3, 39)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 30)\t1\n",
      "  (3, 24)\t1\n",
      "  (3, 33)\t1\n",
      "  (3, 9)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 16)\t1\n",
      "  (4, 31)\t1\n",
      "  (4, 23)\t1\n",
      "  (4, 29)\t1\n",
      "  (4, 30)\t1\n",
      "  (4, 24)\t1\n",
      "  (4, 35)\t1\n",
      "  (4, 17)\t1\n",
      "  (4, 37)\t1\n",
      "  (4, 40)\t2\n",
      "  (4, 38)\t1\n",
      "  (4, 34)\t1\n",
      "  (4, 4)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 26)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 32)\t1\n",
      "  (4, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "# view non-zero feature positions in the sparse matrix\n",
    "print(cv_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding output tells us the total count for each (x, y) pair. Here, x represents a document and y represents a specific word/feature and the value is the number of times y occurs in x. We can leverage the following code to view the output in a dense matrix representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 2, 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view dense representation\n",
    "# warning might give a memory error if data is too big\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, you can see that these documents have been converted into numeric vectors so that each document is represented by one vector (row) in the feature matrix and each column represents a unique word as a feature. The following code represents this in a more easy to understand format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affairs</th>\n",
       "      <th>alike</th>\n",
       "      <th>announced</th>\n",
       "      <th>another</th>\n",
       "      <th>brought</th>\n",
       "      <th>carrying</th>\n",
       "      <th>chance</th>\n",
       "      <th>common</th>\n",
       "      <th>confusion</th>\n",
       "      <th>conscious</th>\n",
       "      <th>...</th>\n",
       "      <th>people</th>\n",
       "      <th>person</th>\n",
       "      <th>position</th>\n",
       "      <th>sense</th>\n",
       "      <th>stray</th>\n",
       "      <th>three</th>\n",
       "      <th>together</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   affairs  alike  announced  another  brought  carrying  chance  common  \\\n",
       "0        0      1          0        0        0         0       0       0   \n",
       "1        0      0          0        0        0         0       0       0   \n",
       "2        0      0          1        0        0         1       0       0   \n",
       "3        1      0          0        0        0         0       0       0   \n",
       "4        0      0          0        1        1         0       1       1   \n",
       "\n",
       "   confusion  conscious  ...  people  person  position  sense  stray  three  \\\n",
       "0          0          0  ...       0       0         0      0      0      0   \n",
       "1          1          0  ...       0       0         0      0      0      0   \n",
       "2          0          0  ...       0       0         0      0      0      0   \n",
       "3          0          1  ...       0       0         1      0      0      1   \n",
       "4          0          0  ...       1       1         0      1      1      0   \n",
       "\n",
       "   together  unhappy  way  wife  \n",
       "0         0        2    1     0  \n",
       "1         0        0    0     0  \n",
       "2         0        0    0     1  \n",
       "3         0        0    0     1  \n",
       "4         2        0    0     0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words n-grams model\n",
    "\n",
    "A word is just a single token, often known as a unigram or 1-gram. We already know that the Bag of Words model doesn’t consider the order of words. But what if we also wanted to take into account phrases or collection of words that occur in a sequence? N-grams help us do that. An N-gram is basically a collection of word tokens from a text document such that these tokens are contiguous and occur in a sequence. Bi-grams indicate n-grams of order 2 (two words), tri-grams indicate n-grams of order 3 (three words), and so on. The Bag of N-Grams model is just an extension of the Bag of Words model that leverages N-gram based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affairs</th>\n",
       "      <th>affairs lasted</th>\n",
       "      <th>alike</th>\n",
       "      <th>alike every</th>\n",
       "      <th>announced</th>\n",
       "      <th>announced husband</th>\n",
       "      <th>another</th>\n",
       "      <th>another members</th>\n",
       "      <th>brought</th>\n",
       "      <th>brought together</th>\n",
       "      <th>...</th>\n",
       "      <th>together</th>\n",
       "      <th>together chance</th>\n",
       "      <th>together stray</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>unhappy family</th>\n",
       "      <th>unhappy way</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "      <th>wife discovered</th>\n",
       "      <th>wife members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   affairs  affairs lasted  alike  alike every  announced  announced husband  \\\n",
       "0        0               0      1            1          0                  0   \n",
       "1        0               0      0            0          0                  0   \n",
       "2        0               0      0            0          1                  1   \n",
       "3        1               1      0            0          0                  0   \n",
       "4        0               0      0            0          0                  0   \n",
       "\n",
       "   another  another members  brought  brought together  ...  together  \\\n",
       "0        0                0        0                 0  ...         0   \n",
       "1        0                0        0                 0  ...         0   \n",
       "2        0                0        0                 0  ...         0   \n",
       "3        0                0        0                 0  ...         0   \n",
       "4        1                1        1                 1  ...         2   \n",
       "\n",
       "   together chance  together stray  unhappy  unhappy family  unhappy way  way  \\\n",
       "0                0               0        2               1            1    1   \n",
       "1                0               0        0               0            0    0   \n",
       "2                0               0        0               0            0    0   \n",
       "3                0               0        0               0            0    0   \n",
       "4                1               1        0               0            0    0   \n",
       "\n",
       "   wife  wife discovered  wife members  \n",
       "0     0                0             0  \n",
       "1     0                0             0  \n",
       "2     1                1             0  \n",
       "3     1                0             1  \n",
       "4     0                0             0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(1,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us feature vectors for our documents, where each feature consists of a bi-gram representing a sequence of two words and values represent how many times the bi-gram was present for our documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF model \n",
    "\n",
    "There are some potential problems that might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms that occur frequently across all documents and these may tend to overshadow other terms in the feature set. Especially words that don’t occur as frequently, but might be more interesting and effective as features to identify specific categories. \n",
    "\n",
    "This is where TF-IDF comes into the picture. TF-IDF stands for term frequency-inverse document frequency. It’s a combination of two metrics, term frequency (tf) and inverse document frequency (idf). \n",
    "\n",
    "TFIDF = TF x IDF\n",
    "\n",
    "Inverse document frequency denoted by idf is the inverse of the document frequency for each term and is computed by dividing the total number of documents in our corpus by the document frequency for each term and then applying logarithmic scaling to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affairs</th>\n",
       "      <th>alike</th>\n",
       "      <th>announced</th>\n",
       "      <th>another</th>\n",
       "      <th>brought</th>\n",
       "      <th>carrying</th>\n",
       "      <th>chance</th>\n",
       "      <th>common</th>\n",
       "      <th>confusion</th>\n",
       "      <th>conscious</th>\n",
       "      <th>...</th>\n",
       "      <th>people</th>\n",
       "      <th>person</th>\n",
       "      <th>position</th>\n",
       "      <th>sense</th>\n",
       "      <th>stray</th>\n",
       "      <th>three</th>\n",
       "      <th>together</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   affairs  alike  announced  another  brought  carrying  chance  common  \\\n",
       "0     0.00   0.33       0.00     0.00     0.00      0.00    0.00    0.00   \n",
       "1     0.00   0.00       0.00     0.00     0.00      0.00    0.00    0.00   \n",
       "2     0.00   0.00       0.27     0.00     0.00      0.27    0.00    0.00   \n",
       "3     0.32   0.00       0.00     0.00     0.00      0.00    0.00    0.00   \n",
       "4     0.00   0.00       0.00     0.23     0.23      0.00    0.23    0.23   \n",
       "\n",
       "   confusion  conscious  ...  people  person  position  sense  stray  three  \\\n",
       "0       0.00       0.00  ...    0.00    0.00      0.00   0.00   0.00   0.00   \n",
       "1       0.57       0.00  ...    0.00    0.00      0.00   0.00   0.00   0.00   \n",
       "2       0.00       0.00  ...    0.00    0.00      0.00   0.00   0.00   0.00   \n",
       "3       0.00       0.32  ...    0.00    0.00      0.32   0.00   0.00   0.32   \n",
       "4       0.00       0.00  ...    0.23    0.23      0.00   0.23   0.23   0.00   \n",
       "\n",
       "   together  unhappy   way  wife  \n",
       "0      0.00     0.67  0.33  0.00  \n",
       "1      0.00     0.00  0.00  0.00  \n",
       "2      0.00     0.00  0.00  0.22  \n",
       "3      0.00     0.00  0.00  0.26  \n",
       "4      0.46     0.00  0.00  0.00  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
    "tt_matrix = tt.fit_transform(cv_matrix)\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = cv.get_feature_names()\n",
    "pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do this with the n-grams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affairs</th>\n",
       "      <th>affairs lasted</th>\n",
       "      <th>alike</th>\n",
       "      <th>alike every</th>\n",
       "      <th>announced</th>\n",
       "      <th>announced husband</th>\n",
       "      <th>another</th>\n",
       "      <th>another members</th>\n",
       "      <th>brought</th>\n",
       "      <th>brought together</th>\n",
       "      <th>...</th>\n",
       "      <th>together</th>\n",
       "      <th>together chance</th>\n",
       "      <th>together stray</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>unhappy family</th>\n",
       "      <th>unhappy way</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "      <th>wife discovered</th>\n",
       "      <th>wife members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   affairs  affairs lasted  alike  alike every  announced  announced husband  \\\n",
       "0     0.00            0.00   0.25         0.25       0.00               0.00   \n",
       "1     0.00            0.00   0.00         0.00       0.00               0.00   \n",
       "2     0.00            0.00   0.00         0.00       0.19               0.19   \n",
       "3     0.22            0.22   0.00         0.00       0.00               0.00   \n",
       "4     0.00            0.00   0.00         0.00       0.00               0.00   \n",
       "\n",
       "   another  another members  brought  brought together  ...  together  \\\n",
       "0     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "1     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "2     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "3     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "4     0.16             0.16     0.16              0.16  ...      0.33   \n",
       "\n",
       "   together chance  together stray  unhappy  unhappy family  unhappy way  \\\n",
       "0             0.00            0.00      0.5            0.25         0.25   \n",
       "1             0.00            0.00      0.0            0.00         0.00   \n",
       "2             0.00            0.00      0.0            0.00         0.00   \n",
       "3             0.00            0.00      0.0            0.00         0.00   \n",
       "4             0.16            0.16      0.0            0.00         0.00   \n",
       "\n",
       "    way  wife  wife discovered  wife members  \n",
       "0  0.25  0.00             0.00          0.00  \n",
       "1  0.00  0.00             0.00          0.00  \n",
       "2  0.00  0.15             0.19          0.00  \n",
       "3  0.00  0.18             0.00          0.22  \n",
       "4  0.00  0.00             0.00          0.00  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
    "tt_matrix = tt.fit_transform(bv_matrix)\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF VECTORIZER\n",
    "\n",
    "You don’t always need to generate features beforehand using a Bag of Words or count based model before engineering TF-IDF features. The TfidfVectorizer by Scikit-Learn enables us to directly compute the tfidf vectors by taking the raw documents as input and internally computing the term frequencies as well as the inverse document frequencies. This eliminates the need to use CountVectorizer to compute the term frequencies based on the Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affairs</th>\n",
       "      <th>alike</th>\n",
       "      <th>announced</th>\n",
       "      <th>another</th>\n",
       "      <th>brought</th>\n",
       "      <th>carrying</th>\n",
       "      <th>chance</th>\n",
       "      <th>common</th>\n",
       "      <th>confusion</th>\n",
       "      <th>conscious</th>\n",
       "      <th>...</th>\n",
       "      <th>people</th>\n",
       "      <th>person</th>\n",
       "      <th>position</th>\n",
       "      <th>sense</th>\n",
       "      <th>stray</th>\n",
       "      <th>three</th>\n",
       "      <th>together</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   affairs  alike  announced  another  brought  carrying  chance  common  \\\n",
       "0     0.00   0.33       0.00     0.00     0.00      0.00    0.00    0.00   \n",
       "1     0.00   0.00       0.00     0.00     0.00      0.00    0.00    0.00   \n",
       "2     0.00   0.00       0.27     0.00     0.00      0.27    0.00    0.00   \n",
       "3     0.32   0.00       0.00     0.00     0.00      0.00    0.00    0.00   \n",
       "4     0.00   0.00       0.00     0.23     0.23      0.00    0.23    0.23   \n",
       "\n",
       "   confusion  conscious  ...  people  person  position  sense  stray  three  \\\n",
       "0       0.00       0.00  ...    0.00    0.00      0.00   0.00   0.00   0.00   \n",
       "1       0.57       0.00  ...    0.00    0.00      0.00   0.00   0.00   0.00   \n",
       "2       0.00       0.00  ...    0.00    0.00      0.00   0.00   0.00   0.00   \n",
       "3       0.00       0.32  ...    0.00    0.00      0.32   0.00   0.00   0.32   \n",
       "4       0.00       0.00  ...    0.23    0.23      0.00   0.23   0.23   0.00   \n",
       "\n",
       "   together  unhappy   way  wife  \n",
       "0      0.00     0.67  0.33  0.00  \n",
       "1      0.00     0.00  0.00  0.00  \n",
       "2      0.00     0.00  0.00  0.22  \n",
       "3      0.00     0.00  0.00  0.26  \n",
       "4      0.46     0.00  0.00  0.00  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm=\"l2\",\n",
    "                     use_idf=True, smooth_idf=True)\n",
    "\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support is also present for adding n-grams to the feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affairs</th>\n",
       "      <th>affairs lasted</th>\n",
       "      <th>alike</th>\n",
       "      <th>alike every</th>\n",
       "      <th>announced</th>\n",
       "      <th>announced husband</th>\n",
       "      <th>another</th>\n",
       "      <th>another members</th>\n",
       "      <th>brought</th>\n",
       "      <th>brought together</th>\n",
       "      <th>...</th>\n",
       "      <th>together</th>\n",
       "      <th>together chance</th>\n",
       "      <th>together stray</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>unhappy family</th>\n",
       "      <th>unhappy way</th>\n",
       "      <th>way</th>\n",
       "      <th>wife</th>\n",
       "      <th>wife discovered</th>\n",
       "      <th>wife members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   affairs  affairs lasted  alike  alike every  announced  announced husband  \\\n",
       "0     0.00            0.00   0.25         0.25       0.00               0.00   \n",
       "1     0.00            0.00   0.00         0.00       0.00               0.00   \n",
       "2     0.00            0.00   0.00         0.00       0.19               0.19   \n",
       "3     0.22            0.22   0.00         0.00       0.00               0.00   \n",
       "4     0.00            0.00   0.00         0.00       0.00               0.00   \n",
       "\n",
       "   another  another members  brought  brought together  ...  together  \\\n",
       "0     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "1     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "2     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "3     0.00             0.00     0.00              0.00  ...      0.00   \n",
       "4     0.16             0.16     0.16              0.16  ...      0.33   \n",
       "\n",
       "   together chance  together stray  unhappy  unhappy family  unhappy way  \\\n",
       "0             0.00            0.00      0.5            0.25         0.25   \n",
       "1             0.00            0.00      0.0            0.00         0.00   \n",
       "2             0.00            0.00      0.0            0.00         0.00   \n",
       "3             0.00            0.00      0.0            0.00         0.00   \n",
       "4             0.16            0.16      0.0            0.00         0.00   \n",
       "\n",
       "    way  wife  wife discovered  wife members  \n",
       "0  0.25  0.00             0.00          0.00  \n",
       "1  0.00  0.00             0.00          0.00  \n",
       "2  0.00  0.15             0.19          0.00  \n",
       "3  0.00  0.18             0.00          0.22  \n",
       "4  0.00  0.00             0.00          0.00  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., norm=\"l2\",\n",
    "                     use_idf=True, smooth_idf=True, ngram_range = (1,2))\n",
    "\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "vocab = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document similarity \n",
    "\n",
    "Document similarity is the process of using a distance or similarity based metric that can identify how similar a text document is to any other document(s) based on features extracted from the documents, like Bag of Words or TF-IDF.\n",
    "\n",
    "Thus you can see that we can build on top of the TF-IDF-based features we engineered in the previous section and use them to generate new features.\n",
    "\n",
    "Pairwise document similarity in a corpus involves computing document similarity for each pair of documents in a corpus. Thus, if you have C documents in a corpus, you would end up with a C x C matrix, such that each row and column represents the similarity score for a pair of documents.\n",
    "\n",
    "In our analysis, we use perhaps the most popular and widely used similarity metrics—cosine similarity and compare pairwise document similarity—based on their TF-IDF feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>0.017663</td>\n",
       "      <td>0.039665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015099</td>\n",
       "      <td>0.034524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.095966</td>\n",
       "      <td>0.044088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095966</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.106325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.039665</td>\n",
       "      <td>0.072868</td>\n",
       "      <td>0.044088</td>\n",
       "      <td>0.106325</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  1.000000  0.000000  0.015099  0.017663  0.039665\n",
       "1  0.000000  1.000000  0.034524  0.000000  0.072868\n",
       "2  0.015099  0.034524  1.000000  0.095966  0.044088\n",
       "3  0.017663  0.000000  0.095966  1.000000  0.106325\n",
       "4  0.039665  0.072868  0.044088  0.106325  1.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### document clustering with similarity features\n",
    "\n",
    "We have been building a lot of features, but let’s use some of them now for a real-world problem of grouping similar documents! Clustering leverages unsupervised learning to group data points (documents in this scenario) into groups or clusters. \n",
    "\n",
    "Since we already have our similarity features, let’s build the linkage matrix on our sample documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document\\Cluster 1</th>\n",
       "      <th>Document\\Cluster 2</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Cluster Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.2672</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.33328</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.40003</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.42505</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document\\Cluster 1 Document\\Cluster 2 Distance Cluster Size\n",
       "0                  3                  4   1.2672            2\n",
       "1                  2                  5  1.33328            3\n",
       "2                  1                  6  1.40003            4\n",
       "3                  0                  7  1.42505            5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "Z = linkage(similarity_matrix, 'ward')\n",
    "pd.DataFrame(Z, columns=['Document\\Cluster 1', 'Document\\Cluster 2',\n",
    "                         'Distance', 'Cluster Size'], dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7fcb48524ad0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Data point')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(Z)\n",
    "plt.axhline(y=1.0, c=\"k\", ls=\"--\", lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy families are all alike; every unhappy family is unhappy in its own way.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everything was in confusion in the Oblonskys’ house.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on liv...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every person in the house felt that there was no sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  Document  \\\n",
       "0                                                                                                                            Happy families are all alike; every unhappy family is unhappy in its own way.   \n",
       "1                                                                                                                                                     Everything was in confusion in the Oblonskys’ house.   \n",
       "2  The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on liv...   \n",
       "3                     This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.   \n",
       "4  Every person in the house felt that there was no sense in their living together, and that the stray people brought together by chance in any inn had more in common with one another than they, the ...   \n",
       "\n",
       "   ClusterLabel  \n",
       "0             5  \n",
       "1             4  \n",
       "2             3  \n",
       "3             1  \n",
       "4             2  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_dist = 1.0\n",
    "cluster_labels = fcluster(Z, max_dist, criterion=\"distance\")\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obivously, clustered into 5 groups as none of the sentences are similar to each other.\n",
    "But this should give you a good idea of how our TF-IDF features were leveraged to build our similarity features, which in turn helped in clustering our documents. You can use this pipeline in the future for clustering your own documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic modelling\n",
    "\n",
    "The idea of topic models revolves around the process of extracting key themes or concepts from a corpus of documents, which are represented as topics. Each topic can be represented as a bag or collection of words/terms from the document corpus. Together, these terms signify a specific topic, theme, or a concept and each topic can be easily distinguished from other topics by virtue of the semantic meaning conveyed by these terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, often you do end up with overlapping topics based on the data. These concepts can range from simple facts and statements to opinions and outlook. Topic models are extremely useful in summarizing large corpus of text documents to extract and depict key concepts. \n",
    "\n",
    "\n",
    "\n",
    "We use another technique called Latent Dirichlet Allocation (LDA) , which uses a generative probabilistic model where each document consists of a combination of several topics and each term or word can be assigned to a specific topic. \n",
    "\n",
    " Frameworks like Gensim or Scikit-Learn enable us to leverage the LDA model for generating topics. For the purpose of feature engineering, which is the intent of this chapter, you need to remember that when LDA is applied to a document-term matrix (TF-IDF or Bag of Words feature matrix), it is broken into two main components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037628</td>\n",
       "      <td>0.924484</td>\n",
       "      <td>0.037888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.863708</td>\n",
       "      <td>0.069304</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021514</td>\n",
       "      <td>0.956275</td>\n",
       "      <td>0.022212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025894</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>0.947124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.966553</td>\n",
       "      <td>0.016693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         T1        T2        T3\n",
       "0  0.037628  0.924484  0.037888\n",
       "1  0.863708  0.069304  0.066987\n",
       "2  0.021514  0.956275  0.022212\n",
       "3  0.025894  0.026982  0.947124\n",
       "4  0.016754  0.966553  0.016693"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=10000, random_state=0)\n",
    "dt_matrix = lda.fit_transform(cv_matrix)\n",
    "features = pd.DataFrame(dt_matrix, columns=['T1', 'T2', 'T3'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('families alike', 1.3371316095361778), ('discovered', 1.3330020191399403), ('brought', 1.332725497211299), ('common', 1.332725497211299)]\n",
      "\n",
      "[('confusion', 3.3310099069938386), ('discovered', 2.332944328802885), ('girl', 2.332675529665739), ('everything confusion', 2.332574203831305), ('chance inn', 2.3322574849109863), ('girl governess', 2.3318374391727006), ('every', 2.3295564365412678), ('alike every', 1.3326368765918362), ('announced', 1.3326368765918362), ('another', 1.3326368765918362), ('another members', 1.3326368765918362), ('confusion oblonskys', 1.3326368765918362), ('every person', 1.3326368765918362), ('family', 1.3326368765918362), ('family household', 1.3326368765918362), ('family unhappy', 1.3326368765918362), ('felt sense', 1.3326368765918362), ('french', 1.3326368765918362), ('alike', 1.3324221291939646), ('announced husband', 1.3324221291939646), ('carrying', 1.3324221291939646), ('chance', 1.3324221291939646), ('conscious', 1.3324221291939646), ('could', 1.3324221291939646), ('could go', 1.3324221291939646), ('days', 1.3324221291939646), ('every unhappy', 1.3324221291939646), ('affairs lasted', 1.3317490430102468), ('common one', 1.3317490430102468), ('days husband', 1.3317490430102468), ('go', 1.3317490430102468), ('discovered husband', 1.3292184050737617), ('families', 1.3292184050737617), ('families alike', 1.3288520935070463), ('go living', 1.3268319981829544)]\n",
      "\n",
      "[('go living', 1.3390452940730344), ('discovered husband', 1.3367607902574), ('families', 1.3367607902574), ('every', 1.336349703933173), ('confusion', 1.3348126908491975), ('affairs', 1.3329590918300707), ('brought together', 1.3329590918300707), ('carrying intrigue', 1.3329590918300707), ('everything', 1.3329590918300707), ('family announced', 1.3329590918300707), ('felt', 1.3329590918300707), ('french girl', 1.3329590918300707)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt_matrix = lda.components_\n",
    "for topic_weights in tt_matrix:\n",
    "    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n",
    "    topic = sorted(topic, key=lambda x: -x[1])\n",
    "    topic = [item for item in topic if item[1] > 0.6]\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, you can clearly see the three topics are quite distinguishable from each other based on their constituent terms. \n",
    "\n",
    "We will look into topic modelling in more detail later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
